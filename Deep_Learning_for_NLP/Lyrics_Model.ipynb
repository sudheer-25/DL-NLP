{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    Predicting Song Similarity using Deep Neural Networks\n",
    "    <br>\n",
    "    Part 02: Models\n",
    "</center>\n",
    "<p style=\"text-align:right\">\n",
    "    Sudheer Kumar Reddy Beeram\n",
    "    <br>\n",
    "    Sivaraman Lakshmipathy\n",
    "    <br>\n",
    "    Sneha Shet\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Models</b>\n",
    "<br>\n",
    "This Jupyter Notebook contains the source code to train the models using the generated dataset.\n",
    "<br>\n",
    "Note: All models were trained on Google Collab using TPU accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlDHybqNqJg7"
   },
   "source": [
    "<center><b>Loading the lyric pairs and preprocessing the data</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "tNPVJmouFFqT",
    "outputId": "a93042ee-03c7-492c-9ee7-4ad7705acdb8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,sys,csv\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "85t6fXfgtPmE",
    "outputId": "607e0991-4ba9-4958-f8cb-5e8a64b13318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Removing words that occur less than 100 times \n",
    "import csv\n",
    "from collections import defaultdict\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean(string):\n",
    "  string = string.replace('(','')\n",
    "  string = string.replace(')','')\n",
    "  string = remove_stopwords(string)\n",
    "  return string\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    retStr = \"\"\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        if w not in stopWords:\n",
    "            lyrics = [''.join(e.lower() for e in w if e.isalpha())]\n",
    "            retStr += lyrics[0] + \" \"\n",
    "    return retStr\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()   \n",
    "counter = defaultdict(int)\n",
    "lines = open('fullDump.txt').read().splitlines()\n",
    "\n",
    "for sentence in lines:\n",
    "  sentence = clean(sentence)\n",
    "  for word in sentence.split():\n",
    "    counter[word]+=1\n",
    "\n",
    "mx_word_count=counter[max(counter.keys(), key=(lambda k: counter[k]))]\n",
    "min_word_count=counter[min(counter.keys(), key=(lambda k: counter[k]))]\n",
    "\n",
    "eliminate=set()\n",
    "for word,count in counter.items():\n",
    "  if count<100:eliminate.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuoxDKLurzVw"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the lyrics\n",
    "def basic_preprocessor(data):\n",
    "    data['X_left'] = data['X_left'].str.lower() # converting to lower case\n",
    "    data['X_right'] = data['X_right'].str.lower() # converting to lower case\n",
    "    data['X_left'] = data['X_left'].str.replace('(','') # Removing parenthesis\n",
    "    data['X_left'] = data['X_left'].str.replace(')','') # Removing parenthesis\n",
    "    data['X_right'] = data['X_right'].str.replace('(','') # Removing parenthesis\n",
    "    data['X_right'] = data['X_right'].str.replace(')','') # Removing parenthesis\n",
    "    data['X_left'] = data['X_left'].apply(remove_stopwords) # Removing stopwords\n",
    "    data['X_right'] = data['X_right'].apply(remove_stopwords) # Removing stopwords\n",
    "    return data\n",
    "\n",
    "#Reading DataFiles\n",
    "dataset_filepath = \"Project_dataset\"\n",
    "dataset_file = \"final_dataset.csv\"\n",
    "\n",
    "data = pd.read_csv(dataset_file, sep = \"\\t\", encoding = \"utf-8\")\n",
    "data = data.reindex(np.random.permutation(data.index))\n",
    "\n",
    "stopWords = set(stopwords.words('english'))|eliminate\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "data = basic_preprocessor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4lReHKVFFqe"
   },
   "outputs": [],
   "source": [
    "dataCorpus = data['X_left'].copy()\n",
    "dataCorpus = dataCorpus.append(data['X_right'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "gA1BjZjRFFql",
    "outputId": "25d8ba5f-e250-43b0-9042-7832c5da80e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 13419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_left_trackid</th>\n",
       "      <th>X_left</th>\n",
       "      <th>X_right_trackid</th>\n",
       "      <th>X_right</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16112</th>\n",
       "      <td>TRMCJGU128E0791C67</td>\n",
       "      <td>[21, 21, 18, 5656, 18, 366, 558, 202, 115, 717...</td>\n",
       "      <td>TRGJHMO128F42478CC</td>\n",
       "      <td>[130, 669, 511, 2107, 406, 25, 2957, 7, 1965, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>TRFMITB128F92C91AD</td>\n",
       "      <td>[17, 30, 1, 544, 379, 14, 14, 58, 6, 3445, 1, ...</td>\n",
       "      <td>TRPTPKD128F426A91D</td>\n",
       "      <td>[13, 38, 1, 28, 111, 1, 1207, 1762, 43, 25, 14...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21111</th>\n",
       "      <td>TRWGQDF128F92E4F04</td>\n",
       "      <td>[354, 4082, 1054, 484, 5657, 753, 9, 611, 182,...</td>\n",
       "      <td>TRWMAFQ128F932E475</td>\n",
       "      <td>[404, 404, 13, 53, 2675, 262, 1572, 53, 462, 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17165</th>\n",
       "      <td>TRYGTCS128F93146AA</td>\n",
       "      <td>[49, 2, 13, 78, 39, 2, 246, 18, 252, 2, 18, 52...</td>\n",
       "      <td>TRRSFKQ128F1484AB8</td>\n",
       "      <td>[731, 64, 80, 658, 397, 2412, 88, 54, 28, 94, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21035</th>\n",
       "      <td>TRWDNTC128F92E87C9</td>\n",
       "      <td>[137, 33, 1029, 85, 288, 943, 133, 122, 13, 45...</td>\n",
       "      <td>TRATBUH128EF34066F</td>\n",
       "      <td>[165, 351, 202, 51, 370, 264, 392, 3384, 350, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X_left_trackid  ...  Y\n",
       "16112  TRMCJGU128E0791C67  ...  0\n",
       "6919   TRFMITB128F92C91AD  ...  1\n",
       "21111  TRWGQDF128F92E4F04  ...  0\n",
       "17165  TRYGTCS128F93146AA  ...  0\n",
       "21035  TRWDNTC128F92E87C9  ...  0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizing the sentences into integer indices\n",
    "def getEmbeddedData(data, tokenizerObj):\n",
    "    return tokenizerObj.texts_to_sequences(data)\n",
    "\n",
    "word_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "word_tokenizer.fit_on_texts(dataCorpus)\n",
    "\n",
    "# Dictionary of all the unique words and their indices in the order of their occuring frequency\n",
    "word_index = word_tokenizer.word_index\n",
    "\n",
    "# No of unique words\n",
    "vocab_length = len(word_index) + 1\n",
    " \n",
    "print(\"Vocabulary length:\", vocab_length)\n",
    "\n",
    "data['X_left'] = getEmbeddedData(data['X_left'], word_tokenizer)\n",
    "data['X_right'] = getEmbeddedData(data['X_right'], word_tokenizer)\n",
    "\n",
    "#Changing Y label to 1 if >0.5 and to 0 otherwise\n",
    "data['Y']=data['Y'].where(data['Y']<=0.5,1)\n",
    "data['Y']=data['Y'].where(data['Y']>0.5,0)\n",
    "data['Y']=data['Y'].astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "8ONbgr8ZFFqq",
    "outputId": "a7ab43cd-61bf-4d16-ec1b-d03a5732d817"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 255\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#Max Length of a sentence \n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(dataCorpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "print(\"Max length:\", length_long_sentence)\n",
    "\n",
    "dataCorpus_1 = data['X_left'].copy()\n",
    "dataCorpus_1 = dataCorpus_1.append(data['X_right'].copy())\n",
    "k =dataCorpus_1.tolist()\n",
    "f = [len(g) for g in k ]\n",
    "max_len = max(f)\n",
    "x = statistics.mean(f)\n",
    "\n",
    "#Mean length was 38.2 and stand dev = 22 hence restricting max seq length to 60\n",
    "sequence_length = 60\n",
    "data2 = data.copy()\n",
    "data2['X_left'] = pad_sequences(data['X_left'], sequence_length, padding='post').tolist()\n",
    "data2['X_right'] = pad_sequences(data['X_right'], sequence_length, padding='post').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "Bu6OS2izVKMg",
    "outputId": "0bab0203-038f-4c9f-e430-ef4c112d327a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_left_trackid</th>\n",
       "      <th>X_left</th>\n",
       "      <th>X_right_trackid</th>\n",
       "      <th>X_right</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16112</th>\n",
       "      <td>TRMCJGU128E0791C67</td>\n",
       "      <td>[21, 21, 18, 5656, 18, 366, 558, 202, 115, 717...</td>\n",
       "      <td>TRGJHMO128F42478CC</td>\n",
       "      <td>[130, 669, 511, 2107, 406, 25, 2957, 7, 1965, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>TRFMITB128F92C91AD</td>\n",
       "      <td>[17, 30, 1, 544, 379, 14, 14, 58, 6, 3445, 1, ...</td>\n",
       "      <td>TRPTPKD128F426A91D</td>\n",
       "      <td>[13, 38, 1, 28, 111, 1, 1207, 1762, 43, 25, 14...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21111</th>\n",
       "      <td>TRWGQDF128F92E4F04</td>\n",
       "      <td>[354, 4082, 1054, 484, 5657, 753, 9, 611, 182,...</td>\n",
       "      <td>TRWMAFQ128F932E475</td>\n",
       "      <td>[404, 404, 13, 53, 2675, 262, 1572, 53, 462, 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17165</th>\n",
       "      <td>TRYGTCS128F93146AA</td>\n",
       "      <td>[49, 2, 13, 78, 39, 2, 246, 18, 252, 2, 18, 52...</td>\n",
       "      <td>TRRSFKQ128F1484AB8</td>\n",
       "      <td>[731, 64, 80, 658, 397, 2412, 88, 54, 28, 94, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21035</th>\n",
       "      <td>TRWDNTC128F92E87C9</td>\n",
       "      <td>[137, 33, 1029, 85, 288, 943, 133, 122, 13, 45...</td>\n",
       "      <td>TRATBUH128EF34066F</td>\n",
       "      <td>[165, 351, 202, 51, 370, 264, 392, 3384, 350, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X_left_trackid  ...  Y\n",
       "16112  TRMCJGU128E0791C67  ...  0\n",
       "6919   TRFMITB128F92C91AD  ...  1\n",
       "21111  TRWGQDF128F92E4F04  ...  0\n",
       "17165  TRYGTCS128F93146AA  ...  0\n",
       "21035  TRWDNTC128F92E87C9  ...  0\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCRWPOazFFqv"
   },
   "outputs": [],
   "source": [
    "#Splitting into train/test/dev and adding (b,a) pairs\n",
    "validationDataSize = 3500\n",
    "testDataSize = 3500\n",
    "trainingDataSize = data2.shape[0] - validationDataSize - testDataSize\n",
    "\n",
    "#Train data\n",
    "trainData_similar = data2[:trainingDataSize//2]\n",
    "trainData_non_similar = data2[data2.shape[0]//2:data2.shape[0]//2 + trainingDataSize//2]\n",
    "\n",
    "trainData = pd.concat([trainData_similar,trainData_non_similar],ignore_index=True, sort = False)\n",
    "\n",
    "trainData1 = pd.DataFrame({'X_left_trackid': trainData['X_right_trackid'],'X_left':trainData['X_right'],'X_right_trackid': trainData['X_left_trackid'],'X_right':trainData['X_left'],'Y':trainData['Y']})\n",
    "trainData_both = pd.concat([trainData,trainData1],ignore_index=True, sort = False)\n",
    "trainData_both= trainData_both.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Validation Data\n",
    "validationData_similar = data2[trainingDataSize//2:trainingDataSize//2 + validationDataSize//2]\n",
    "validationData_non_similar = data2[data2.shape[0]//2 + trainingDataSize//2:data2.shape[0]//2 + trainingDataSize//2 + validationDataSize//2]\n",
    "\n",
    "validationData = pd.concat([validationData_similar,validationData_non_similar],ignore_index=True, sort = False)\n",
    "\n",
    "validationData1 = pd.DataFrame({'X_left_trackid': validationData['X_right_trackid'],'X_left':validationData['X_right'],'X_right_trackid': validationData['X_left_trackid'],'X_right':validationData['X_left'],'Y':validationData['Y']})\n",
    "validationData_both = pd.concat([validationData,validationData1],ignore_index=True, sort = False)\n",
    "validationData_both= validationData_both.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Test Data\n",
    "testData_similar = data2[(trainingDataSize + validationDataSize)//2:data2.shape[0]//2]\n",
    "testData_non_similar = data2[data2.shape[0] - testDataSize//2:data2.shape[0]]\n",
    "\n",
    "testData = pd.concat([testData_similar,testData_non_similar],ignore_index=True, sort = False)\n",
    "\n",
    "testData1 = pd.DataFrame({'X_left_trackid': testData['X_right_trackid'],'X_left':testData['X_right'],'X_right_trackid': testData['X_left_trackid'],'X_right':testData['X_left'],'Y':testData['Y']})\n",
    "testData_both = pd.concat([testData,testData1],ignore_index=True, sort = False)\n",
    "testData_both= testData_both.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bFZF-0CFFq5"
   },
   "outputs": [],
   "source": [
    "#Using Pretrained embeddings\n",
    "embeddings_index = dict()\n",
    "f = open(r'lyrics_model_100dim.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    if index > vocab_length - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWTTFeeYw-yC"
   },
   "source": [
    "<center><b>Training and Evaluating the Models</b></center>\n",
    "<br>\n",
    "<b>1. Siamese [Base Line]</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gLhW75w2NLj"
   },
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iGyErBHs2dx6"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "X_train_left = np.array(trainData_both['X_left'].tolist())\n",
    "X_train_right = np.array(trainData_both['X_right'].tolist())\n",
    "X_valid_left = np.array(validationData_both['X_left'].tolist())\n",
    "X_valid_right = np.array(validationData_both['X_right'].tolist())\n",
    "Y_train = np.array(trainData_both['Y'].tolist())\n",
    "Y_valid = np.array(validationData_both['Y'].tolist())\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IlfhSLRhyYzg",
    "outputId": "7d61171d-ecbd-4e67-c697-08107265b945",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_A (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_B (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 60, 100)      1341900     Input_A[0][0]                    \n",
      "                                                                 Input_B[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 60)           38640       embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,380,540\n",
      "Trainable params: 1,380,540\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 34040 samples, validate on 7000 samples\n",
      "Epoch 1/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.3475 - acc: 0.5092 - val_loss: 0.3299 - val_acc: 0.5031\n",
      "Epoch 2/25\n",
      "34040/34040 [==============================] - 91s 3ms/step - loss: 0.3062 - acc: 0.5221 - val_loss: 0.3005 - val_acc: 0.5103\n",
      "Epoch 3/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.2763 - acc: 0.5339 - val_loss: 0.2836 - val_acc: 0.5046\n",
      "Epoch 4/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.2608 - acc: 0.5523 - val_loss: 0.2880 - val_acc: 0.4954\n",
      "Epoch 5/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.2513 - acc: 0.5702 - val_loss: 0.2865 - val_acc: 0.4986\n",
      "Epoch 6/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2454 - acc: 0.5866 - val_loss: 0.2817 - val_acc: 0.5034\n",
      "Epoch 7/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2394 - acc: 0.6004 - val_loss: 0.2806 - val_acc: 0.5037\n",
      "Epoch 8/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2334 - acc: 0.6144 - val_loss: 0.2846 - val_acc: 0.4989\n",
      "Epoch 9/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2286 - acc: 0.6295 - val_loss: 0.2834 - val_acc: 0.4860\n",
      "Epoch 10/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2244 - acc: 0.6387 - val_loss: 0.2844 - val_acc: 0.4900\n",
      "Epoch 11/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.2209 - acc: 0.6469 - val_loss: 0.2857 - val_acc: 0.4971\n",
      "Epoch 12/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2165 - acc: 0.6591 - val_loss: 0.2813 - val_acc: 0.4937\n",
      "Epoch 13/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2134 - acc: 0.6683 - val_loss: 0.2822 - val_acc: 0.4909\n",
      "Epoch 14/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2101 - acc: 0.6752 - val_loss: 0.2827 - val_acc: 0.4909\n",
      "Epoch 15/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.2074 - acc: 0.6854 - val_loss: 0.2853 - val_acc: 0.4931\n",
      "Epoch 16/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.2042 - acc: 0.6934 - val_loss: 0.2814 - val_acc: 0.4851\n",
      "Epoch 17/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.2020 - acc: 0.6973 - val_loss: 0.2816 - val_acc: 0.4851\n",
      "Epoch 18/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.1992 - acc: 0.7074 - val_loss: 0.2859 - val_acc: 0.5017\n",
      "Epoch 19/25\n",
      "34040/34040 [==============================] - 89s 3ms/step - loss: 0.1965 - acc: 0.7129 - val_loss: 0.2814 - val_acc: 0.4866\n",
      "Epoch 20/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.1942 - acc: 0.7193 - val_loss: 0.2852 - val_acc: 0.4911\n",
      "Epoch 21/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.1917 - acc: 0.7266 - val_loss: 0.2815 - val_acc: 0.5006\n",
      "Epoch 22/25\n",
      "34040/34040 [==============================] - 90s 3ms/step - loss: 0.1897 - acc: 0.7297 - val_loss: 0.2821 - val_acc: 0.4920\n",
      "Epoch 23/25\n",
      "34040/34040 [==============================] - 91s 3ms/step - loss: 0.1872 - acc: 0.7374 - val_loss: 0.2801 - val_acc: 0.4937\n",
      "Epoch 24/25\n",
      "34040/34040 [==============================] - 91s 3ms/step - loss: 0.1851 - acc: 0.7403 - val_loss: 0.2808 - val_acc: 0.4931\n",
      "Epoch 25/25\n",
      "34040/34040 [==============================] - 91s 3ms/step - loss: 0.1824 - acc: 0.7483 - val_loss: 0.2815 - val_acc: 0.4977\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Lambda\n",
    "import keras.backend as K\n",
    "from keras.models import Input,Model\n",
    "from keras import regularizers\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import Dense, Dropout,Embedding, Activation,Concatenate,LSTM,Subtract,Multiply, GRU\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "max_input_length = sequence_length\n",
    "vocabulary_size = vocab_length\n",
    "\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 25\n",
    "\n",
    "def Train_Siamese(learning_rate):\n",
    "    \n",
    "    global max_input_length,vocabulary_size  \n",
    "    embedding_layer = Embedding(vocabulary_size,\n",
    "                                embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_input_length,\n",
    "                                trainable=True,\n",
    "                                name = \"embedding\")\n",
    "\n",
    "    sequence_input_A = Input(shape=(max_input_length,), dtype='int32',name=\"Input_A\")\n",
    "    sequence_input_B = Input(shape=(max_input_length,), dtype='int32',name=\"Input_B\")\n",
    "\n",
    "    embedded_sequences_A = embedding_layer(sequence_input_A)\n",
    "    embedded_sequences_B = embedding_layer(sequence_input_B)\n",
    "\n",
    "    lstm = LSTM(max_input_length)\n",
    "\n",
    "    hfinal_A = lstm(embedded_sequences_A)\n",
    "    hfinal_B = lstm(embedded_sequences_B)\n",
    "    \n",
    "    # Calculates the distance as defined by the MaLSTM model\n",
    "    malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),\n",
    "                             output_shape=lambda x: (x[0][0], 1))([hfinal_A, hfinal_B])\n",
    "\n",
    "    # Pack it all up into a model\n",
    "    malstm = Model([sequence_input_A, sequence_input_B], [malstm_distance])\n",
    "\n",
    "    # Adadelta optimizer, with gradient clipping by norm\n",
    "    optimizer = optimizers.Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "    malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "    malstm.summary()\n",
    "    \n",
    "    malstm_trained = malstm.fit([X_train_left, X_train_right], Y_train, \n",
    "                                batch_size=batch_size, \n",
    "                                nb_epoch=n_epoch,\n",
    "                                validation_data=([X_valid_left, X_valid_right], Y_valid))\n",
    "    return malstm\n",
    "    \n",
    "malstm_trained = Train_Siamese(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "t--H4HgV0Mqs",
    "outputId": "f2e1e8c3-b056-40be-daac-ecdc43f939e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 5s 650us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28533432408741544, 0.48228571428571426]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate\n",
    "X_test_left = np.array(testData_both['X_left'].tolist())\n",
    "X_test_right = np.array(testData_both['X_right'].tolist())\n",
    "Y_test = np.array(testData_both['Y'].tolist())\n",
    "malstm_trained.evaluate([X_test_left,X_test_right], Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUWLwalR1HMj"
   },
   "source": [
    "<b>2. Twin Architecture [LSTM]</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zhk8Am1_FFq8"
   },
   "outputs": [],
   "source": [
    "def Train_LSTM(learning_rate,combine_type,output_units):\n",
    "    \n",
    "    global max_input_length,vocabulary_size\n",
    "    random_weights = RandomNormal(mean=0.0, stddev=0.05, seed=100)\n",
    "\n",
    "\n",
    "    embedding_layer = Embedding(vocabulary_size,\n",
    "                                embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_input_length,\n",
    "                                trainable=True,\n",
    "                                name = \"embedding\")\n",
    "\n",
    "    sequence_input_A = Input(shape=(max_input_length,), dtype='int32',name=\"Input_A\")\n",
    "    sequence_input_B = Input(shape=(max_input_length,), dtype='int32',name=\"Input_B\")\n",
    "\n",
    "    embedded_sequences_A = embedding_layer(sequence_input_A)\n",
    "    embedded_sequences_B = embedding_layer(sequence_input_B)\n",
    "\n",
    "    lstm = LSTM(output_units, return_sequences=False, \n",
    "                  dropout=0.1, recurrent_dropout=0.1)\n",
    "\n",
    "    hfinal_A = lstm(embedded_sequences_A)\n",
    "    hfinal_B = lstm(embedded_sequences_B)\n",
    "\n",
    "\n",
    "    if combine_type==\"Subtract\":\n",
    "        combined = Subtract(name=\"Subtract\")([hfinal_A, hfinal_B])\n",
    "    elif combine_type == \"Multiply\":\n",
    "        combined = Multiply(name=\"Multiply\")([hfinal_A, hfinal_B])\n",
    "    elif combine_type==\"Concatenate\":\n",
    "        combined = Concatenate(axis=-1,name=\"Concatenate\")([hfinal_A, hfinal_B])\n",
    "\n",
    "    model = Dropout(0.1,name=\"Dropout_layer_1\")(combined)\n",
    "    model = Dense(64,activation='relu',name=\"Dense_1\")(model)\n",
    "    model = Dense(1,activation='sigmoid',name=\"Ouput_layer\")(model)\n",
    "    final_model = Model(inputs=[sequence_input_A,sequence_input_B], outputs=model)\n",
    "\n",
    "    #optimizers\n",
    "    Adam=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "    final_model.compile(loss='binary_crossentropy',optimizer=Adam,metrics=['accuracy'])\n",
    "    final_model.summary()\n",
    "    print('......................................................')\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "colab_type": "code",
    "id": "2GiE52xZFFrB",
    "outputId": "56dfb30c-3cbf-478f-967f-b2e3f3716938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_A (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_B (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 60, 100)      1341900     Input_A[0][0]                    \n",
      "                                                                 Input_B[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           42240       embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Multiply (Multiply)             (None, 64)           0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_layer_1 (Dropout)       (None, 64)           0           Multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, 64)           4160        Dropout_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Ouput_layer (Dense)             (None, 1)            65          Dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,388,365\n",
      "Trainable params: 1,388,365\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "......................................................\n",
      "Train on 34040 samples, validate on 7000 samples\n",
      "Epoch 1/25\n",
      "34040/34040 [==============================] - 104s 3ms/step - loss: 0.6937 - acc: 0.5069 - val_loss: 0.6936 - val_acc: 0.4986\n",
      "Epoch 2/25\n",
      "34040/34040 [==============================] - 102s 3ms/step - loss: 0.6897 - acc: 0.5227 - val_loss: 0.7071 - val_acc: 0.4951\n",
      "Epoch 3/25\n",
      "34040/34040 [==============================] - 103s 3ms/step - loss: 0.6774 - acc: 0.5301 - val_loss: 0.7200 - val_acc: 0.5011\n",
      "Epoch 4/25\n",
      "34040/34040 [==============================] - 102s 3ms/step - loss: 0.6686 - acc: 0.5412 - val_loss: 0.7338 - val_acc: 0.5006\n",
      "Epoch 5/25\n",
      "34040/34040 [==============================] - 102s 3ms/step - loss: 0.6637 - acc: 0.5398 - val_loss: 0.7528 - val_acc: 0.4989\n",
      "Epoch 6/25\n",
      "34040/34040 [==============================] - 102s 3ms/step - loss: 0.6567 - acc: 0.5505 - val_loss: 0.7536 - val_acc: 0.4914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f33aebc89e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model\n",
    "final_model_LSTM = Train_LSTM(0.01,\"Multiply\",64)\n",
    "final_model_LSTM.fit([X_train_left,X_train_right], Y_train, batch_size=64, epochs=25, validation_data = ([X_valid_left ,X_valid_right],Y_valid ), callbacks = [early_stop])#, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "yMZ8AP4R28aQ",
    "outputId": "2a90773a-5902-468d-989a-c2ff8e8153b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 5s 695us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7678706512451172, 0.49914285710879736]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "final_model_LSTM.evaluate([X_test_left,X_test_right], Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8afxYMi3SLt"
   },
   "source": [
    "<b>3.Twin Architecture [GRU]</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKAgguS5a5oM"
   },
   "outputs": [],
   "source": [
    "def Train_GRU(learning_rate,combine_type,output_units):\n",
    "    \n",
    "    global max_input_length,vocabulary_size\n",
    "    embedding_layer = Embedding(vocabulary_size,\n",
    "                                embedding_matrix.shape[1],\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_input_length,\n",
    "                                trainable=True,\n",
    "                                name = \"embedding\")\n",
    "\n",
    "    sequence_input_A = Input(shape=(max_input_length,), dtype='int32',name=\"Input_A\")\n",
    "    sequence_input_B = Input(shape=(max_input_length,), dtype='int32',name=\"Input_B\")\n",
    "\n",
    "    embedded_sequences_A = embedding_layer(sequence_input_A)\n",
    "    embedded_sequences_B = embedding_layer(sequence_input_B)\n",
    "\n",
    "    gru = GRU(embedding_matrix.shape[1], return_sequences=False, \n",
    "                  dropout=0.1, recurrent_dropout=0.1)\n",
    "\n",
    "    hfinal_A = gru(embedded_sequences_A)\n",
    "    hfinal_B = gru(embedded_sequences_B)\n",
    "\n",
    "\n",
    "    if combine_type==\"Subtract\":\n",
    "        combined = Subtract(name=\"Subtract\")([hfinal_A, hfinal_B])\n",
    "    elif combine_type == \"Multiply\":\n",
    "        combined = Multiply(name=\"Multiply\")([hfinal_A, hfinal_B])\n",
    "    elif combine_type==\"Concatenate\":\n",
    "        combined = Concatenate(axis=-1,name=\"Concatenate\")([hfinal_A, hfinal_B])\n",
    "\n",
    "    model = Dropout(0.1,name=\"Dropout_layer_1\")(combined)\n",
    "    model = Dense(64,activation='relu',name=\"Dense_1\")(model)\n",
    "    model = Dense(1,activation='sigmoid',name=\"Ouput_layer\")(model)\n",
    "    final_model = Model(inputs=[sequence_input_A,sequence_input_B], outputs=model)\n",
    "\n",
    "\n",
    "    #optimizers\n",
    "    Adam=optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "    final_model.compile(loss='binary_crossentropy',optimizer=Adam,metrics=['accuracy'])\n",
    "    final_model.summary()\n",
    "    print('......................................................')\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "colab_type": "code",
    "id": "js-h_bzQ5h4q",
    "outputId": "26d22769-076e-4a85-9c8d-03140037acc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input_A (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input_B (InputLayer)            (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 60, 100)      1341900     Input_A[0][0]                    \n",
      "                                                                 Input_B[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 100)          60300       embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Multiply (Multiply)             (None, 100)          0           gru_1[0][0]                      \n",
      "                                                                 gru_1[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dropout_layer_1 (Dropout)       (None, 100)          0           Multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, 64)           6464        Dropout_layer_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Ouput_layer (Dense)             (None, 1)            65          Dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,408,729\n",
      "Trainable params: 1,408,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "......................................................\n",
      "Train on 34040 samples, validate on 7000 samples\n",
      "Epoch 1/25\n",
      "34040/34040 [==============================] - 108s 3ms/step - loss: 0.6951 - acc: 0.4974 - val_loss: 0.6933 - val_acc: 0.4980\n",
      "Epoch 2/25\n",
      "34040/34040 [==============================] - 106s 3ms/step - loss: 0.6934 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.4969\n",
      "Epoch 3/25\n",
      "34040/34040 [==============================] - 106s 3ms/step - loss: 0.6934 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4969\n",
      "Epoch 4/25\n",
      "34040/34040 [==============================] - 106s 3ms/step - loss: 0.6933 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5031\n",
      "Epoch 5/25\n",
      "34040/34040 [==============================] - 106s 3ms/step - loss: 0.6951 - acc: 0.5002 - val_loss: 0.6938 - val_acc: 0.4963\n",
      "Epoch 6/25\n",
      "34040/34040 [==============================] - 107s 3ms/step - loss: 0.6935 - acc: 0.5008 - val_loss: 0.6934 - val_acc: 0.4966\n",
      "Epoch 7/25\n",
      "34040/34040 [==============================] - 107s 3ms/step - loss: 0.6941 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4969\n",
      "Epoch 8/25\n",
      "34040/34040 [==============================] - 108s 3ms/step - loss: 0.6932 - acc: 0.5018 - val_loss: 0.6937 - val_acc: 0.4963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f33a08f6518>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the model\n",
    "final_model_GRU = Train_GRU(0.01,\"Multiply\",64)\n",
    "final_model_GRU.fit([X_train_left,X_train_right], Y_train, batch_size=64, epochs=25, validation_data = ([X_valid_left ,X_valid_right],Y_valid ), callbacks = [early_stop])#, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3mby9Wkp5ltu",
    "outputId": "9d2b7a82-52dd-4121-b9a5-848a2cceeddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 6s 829us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6937591751643589, 0.49514285714285716]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "final_model_GRU.evaluate([X_test_left,X_test_right], Y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS_594_Project_Model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

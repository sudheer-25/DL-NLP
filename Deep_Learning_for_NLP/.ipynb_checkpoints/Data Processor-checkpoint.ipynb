{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    Predicting Song Similarity using Deep Neural Networks\n",
    "    <br>\n",
    "    Part 01: Data Processor\n",
    "</center>\n",
    "<p style=\"text-align:right\">\n",
    "    Sneha Shet\n",
    "    <br>\n",
    "    Sivaraman Lakshmipathy\n",
    "    <br>\n",
    "    Sudheer Kumar Reddy Beeram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Processor:</b>\n",
    "<br>\n",
    "This Jupyter Notebook contains the source code that is used to\n",
    "<ul>\n",
    "    <li>download the lyrics data corpus</li>\n",
    "    <li>filter it to identify the data set to run experiments</li>\n",
    "    <li>generate custom word embeddings to be used for the experiments</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Data download and initial operations</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 1: Downloading the data corpus</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. The initial step is to download the similarity ground truth, available at http://millionsongdataset.com/lastfm\n",
    "\n",
    "    Extract the contents and ensure that 'lastfm_similars.db' is available at the root level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02. The second step is to download the Musixmatch data in order to identify the mapping ids that relate the ground truth to the actual track ids. The same can be found here: http://millionsongdataset.com/musixmatch\n",
    "\n",
    "     Extract the contents and ensure that the following files are available:<br>\n",
    "     • Musixmatch\\mxm_dataset_train.txt<br>\n",
    "     • Musixmatch\\mxm_dataset_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03. Executing the following query on the <b>lastfm_similars.db</b> provided the list of all unique track ids in the data corpus, which is saved as a CSV file named <b>unique_track_ids.csv</b>\n",
    "\n",
    "    SELECT  s.tid,d.tid<br>\n",
    "    FROM similars_dest d<br>\n",
    "    LEFT JOIN similars_src s USING(tid)<br>\n",
    "    UNION ALL<br>\n",
    "    SELECT  s.tid,d.tid<br>\n",
    "    FROM similars_src s<br>\n",
    "    LEFT JOIN similars_dest d USING(tid)<br>\n",
    "    WHERE d.tid IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04. Extract the track ids and download the corresponding lyrics from Musixmatch databases. A developer API key is required for this operation and a sample working key is provided here which expires once 2019 ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate dict for Trackid : Musixmatch id\n",
    "\n",
    "import json\n",
    "\n",
    "dictObj = {}\n",
    "\n",
    "mxm_train_file = \"Musixmatch\\mxm_dataset_train.txt\"\n",
    "mxm_test_file = \"Musixmatch\\mxm_dataset_test.txt\"\n",
    "\n",
    "def processFile(fileName, dictObj):\n",
    "    f = open(fileName, 'r')\n",
    "    for entry in f:\n",
    "        if not entry.startswith('#') and not entry.startswith('%'):\n",
    "            splitEntry = entry.split(',')\n",
    "            trackId = splitEntry[0]\n",
    "            mxmId = splitEntry[1]\n",
    "            \n",
    "            #sanity check\n",
    "            if not trackId.startswith('TR'):\n",
    "                print(\"Error!\")\n",
    "                break\n",
    "                \n",
    "            if trackId in dictObj:\n",
    "                print(\"Track id already present!\")\n",
    "                print(trackId, \":\", mxmId)\n",
    "            else:\n",
    "                dictObj[trackId] = mxmId\n",
    "    f.close()\n",
    "    return dictObj\n",
    "        \n",
    "dictObj = processFile(mxm_train_file, dictObj)\n",
    "dictObj = processFile(mxm_test_file, dictObj)\n",
    "\n",
    "writeToFile = open(\"track_mxm_map.txt\", 'w')\n",
    "writeToFile.write(json.dumps(dictObj))\n",
    "writeToFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get trackids extracted from CSV\n",
    "\n",
    "import csv\n",
    "\n",
    "trackIdList = []\n",
    "\n",
    "with open('unique_track_ids.csv') as trackidFile:\n",
    "    trackFile = csv.reader(trackidFile, delimiter=',')\n",
    "    for row in trackFile:\n",
    "        if row[0] != '':\n",
    "            trackIdList.append(row[0])\n",
    "        else:\n",
    "            trackIdList.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get mxmids\n",
    "\n",
    "trackIds = []\n",
    "mxmIdList = []\n",
    "\n",
    "f = open(\"mxm_download.txt\", \"w\")\n",
    "f.write(\"#mxmId, #trackid per line\\n\")\n",
    "f_mxm_unavailable = open(\"mxm_unavailable.txt\", \"w\")\n",
    "\n",
    "for entry in trackIdList:\n",
    "    if entry in dictObj:\n",
    "        tId = entry\n",
    "        mxmId = dictObj[entry]\n",
    "        fileStr = mxmId + \",\" + tId + \"\\n\"\n",
    "        f.write(fileStr)\n",
    "    else:\n",
    "        f_mxm_unavailable.write(entry + \"\\n\")\n",
    "        \n",
    "f.close()\n",
    "f_mxm_unavailable.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API call to musixmatch\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "\n",
    "def checkIfLyricsAvailable(trackId):\n",
    "    fullJsonPath = \"Lyrics_dump\\\\Full_json\\\\\"\n",
    "    lyricsPath = \"Lyrics_dump\\\\Lyrics\\\\\"\n",
    "    if os.path.isfile(fullJsonPath + trackId + \".json\") and os.path.isfile(lyricsPath + trackId + \".txt\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def getLyrics(api_key, mxmId, trackId):\n",
    "    \n",
    "    #Return status key\n",
    "    #200 - Lyrics available / downloaded successfully\n",
    "    #402 - Change API\n",
    "    #400 - Error\n",
    "    \n",
    "    if checkIfLyricsAvailable(trackId):\n",
    "        return 200\n",
    "    \n",
    "    copyright_text = \"******* This Lyrics is NOT for Commercial use *******\" #to remove text from lyrics\n",
    "    \n",
    "    url = \"https://api.musixmatch.com/ws/1.1/track.lyrics.get\"\n",
    "    url_params = {\n",
    "        \"apikey\": api_key,\n",
    "        \"track_id\": mxmId\n",
    "    }\n",
    "    result = requests.get(url, params=url_params)\n",
    "    res_json = result.json()\n",
    "    status_code = res_json[\"message\"][\"header\"][\"status_code\"]\n",
    "    \n",
    "    if status_code == 200: #successful request\n",
    "        try:\n",
    "            lyrics = res_json[\"message\"][\"body\"][\"lyrics\"][\"lyrics_body\"]\n",
    "            if lyrics == \"\":\n",
    "                unavailFile = open(\"lyrics_unavailable_list.txt\", \"a\")\n",
    "                unavailFile.write(str(mxmId) + \",\" + trackId + \"\\n\")\n",
    "                unavailFile.close()\n",
    "                return 404\n",
    "\n",
    "            if copyright_text in lyrics:\n",
    "                lyrics = lyrics[:lyrics.index(copyright_text)].strip()\n",
    "\n",
    "            f1 = open(\"Lyrics_dump\\\\Full_json\\\\\" + trackId + \".json\", \"w\", encoding=\"utf-8\")\n",
    "            f1.write(json.dumps(res_json[\"message\"][\"body\"]))\n",
    "            f1.close()\n",
    "\n",
    "            f2 = open(\"Lyrics_dump\\\\Lyrics\\\\\" + trackId + \".txt\", \"w\", encoding=\"utf-8\")\n",
    "            f2.write(lyrics)\n",
    "            f2.close()\n",
    "            return 200\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return 400\n",
    "    elif status_code == 401 or status_code == 402: #API hit limit reached\n",
    "        print(\"Time to swap api_key\")\n",
    "        return 402\n",
    "    elif status_code == 404: #Lyrics not found\n",
    "        unavailFile = open(\"lyrics_unavailable_list.txt\", \"a\")\n",
    "        unavailFile.write(str(mxmId) + \",\" + trackId + \"\\n\")\n",
    "        unavailFile.close()\n",
    "        return 404\n",
    "    else:\n",
    "        print(\"Unexpected error for mxmId:\", mxmId, \", trackId:\", trackId)\n",
    "        print(res_json)\n",
    "        return 400\n",
    "        \n",
    "    time.sleep(0.200)\n",
    "\n",
    "#File containing the apikeys\n",
    "f = open(\"mxm_apikey.txt\", \"r\")\n",
    "apikeys = []\n",
    "for entry in f:\n",
    "    apikey = entry.strip()\n",
    "    apikeys.append(apikey)\n",
    "f.close()\n",
    "    \n",
    "i = 0\n",
    "\n",
    "#File containing the mxmids\n",
    "mxmIdFile = open(\"mxm_download.txt\", \"r\")\n",
    "#Skip first line\n",
    "next(mxmIdFile)\n",
    "\n",
    "for line in mxmIdFile:\n",
    "    entries = line.strip().split(\",\")\n",
    "    mxmId = int(entries[0])\n",
    "    trackId = entries[1]\n",
    "    if trackId in tracks_unavailable:\n",
    "        continue\n",
    "    processStatus = getLyrics(apikeys[i], mxmId, trackId)\n",
    "    if processStatus != 200:\n",
    "        if processStatus == 402:\n",
    "            print(\"Updating API key.\")\n",
    "            i += 1\n",
    "            if i == len(apikeys):\n",
    "                print(\"API keys exhausted!\")\n",
    "                break\n",
    "        elif processStatus != 404:\n",
    "            print(\"Unexpected error encountered!\")\n",
    "            break\n",
    "    \n",
    "mxmIdFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Part 2: Data filtering</b>\n",
    "\n",
    "Filter out non-English lyrics and create a dump file containing the entire data corpus. This will be used later for training the word embeddings.\n",
    "<br><br>\n",
    "Note: A Python package named 'langdetect' is required to perform this operation.\n",
    "<br>\n",
    "The final generated file is provided as part of the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package dependency: langdetect\n",
    "#https://pypi.org/project/langdetect\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "lyrics_files_path = \"Lyrics_dump\" + os.path.sep + \"Lyrics\" + os.path.sep\n",
    "\n",
    "#Get the list of all files with lyrics\n",
    "filelist = [f for f in glob.glob(lyrics_files_path + \"*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write list of lyric files in languages other than English to a separate file\n",
    "\n",
    "other_lang_Lyrics_list_file = \"lyrics_otherLang.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherLangFile = open(other_lang_Lyrics_list_file, \"a\")\n",
    "\n",
    "for entry in filelist:\n",
    "    try:\n",
    "        with open(entry, \"r\", encoding=\"utf8\") as f:\n",
    "            curLyrics = f.read()\n",
    "            curLyrics = curLyrics[:-3].replace(\"\\n\", \" \")\n",
    "            lyrics_language = detect(curLyrics)\n",
    "            if lyrics_language != 'en':\n",
    "                otherLangFile.write(entry + \",\" + lyrics_language + \"\\n\")\n",
    "    except Exception as e:\n",
    "        otherLangFile.write(entry + \",\" + \"undefined\" + \"\\n\")\n",
    "\n",
    "otherLangFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Creating Track ID pairs and their scores<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following set of code blocks identify the similar tracks for each unique track id in the database and generate (source, target) pairs along with their respective similarity label. Similarly, tracks with at least 5 similar and non-similar tracks are identified and recorded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# Establishing connection to the database\n",
    "con = sqlite3.connect(\"lastfm_similars.db\")\n",
    "cursorObj = con.cursor()\n",
    "cursorObj.execute(\"SELECT *FROM similars_src\")\n",
    "rows = cursorObj.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the all track ids that doesn't have lyrics \n",
    "f = open(\"lyrics_unavailable_list.txt\",\"r\")\n",
    "lyrics_unavailable=[]\n",
    "for row in f:\n",
    "    lyrics_unavailable.append(row.split(\",\")[1][:-1])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding track ids of songs that are not in English Language to list of Lyrics Unavailable list\n",
    "f_non_English = open(\"lyrics_otherLang_tracks.txt\",\"r\")\n",
    "for row in f_non_English:\n",
    "    lyrics_unavailable.append(row[:-1])\n",
    "f_non_English.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding track ids that are not in mxm database to list of Lyrics Unavailable list\n",
    "f_no_mxm = open(\"mxm_unavailable.txt\", \"r\")\n",
    "for row in f_no_mxm:\n",
    "    lyrics_unavailable.append(row[:-1])\n",
    "f_no_mxm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_unavailable = set(lyrics_unavailable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting target songs row in to similar and non similar songs list\n",
    "def find_mid_split_into_two(target_songs_row):\n",
    "    Left_split=[] # >=0.5\n",
    "    Right_split = [] # <=0.5\n",
    "    k=0\n",
    "    for i in range(len(target_songs_row)):\n",
    "        if target_songs_row[k] not in lyrics_unavailable:\n",
    "            if float(target_songs_row[k+1])>0.5:\n",
    "                Left_split.append([target_songs_row[k],target_songs_row[k+1]])\n",
    "            else:\n",
    "                Right_split.append([target_songs_row[k],target_songs_row[k+1]])\n",
    "        k = k+2\n",
    "        if k > len(target_songs_row)-2:\n",
    "            return Left_split, Right_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating top 5 most similar and 5 non similar target track IDs and their similarity score with source songs\n",
    "import random\n",
    "f_similar= open(\"Pairs_labels_similar.txt\",\"w\")\n",
    "f_non_similar = open(\"Pairs_labels_non_similar.txt\",\"w\")\n",
    "\n",
    "Target_row_lengths=[]\n",
    "for row in rows:\n",
    "    ID_L = row[0]\n",
    "    if ID_L not in lyrics_unavailable:\n",
    "        splitted_row = row[1].split(\",\")\n",
    "        Left_split = find_mid_split_into_two(splitted_row)[0]\n",
    "        Right_split = find_mid_split_into_two(splitted_row)[1]\n",
    "        Target_row_lengths.append([len(Left_split),len(Right_split)])\n",
    "        if len(Left_split) >=5 and len(Right_split) >= 5:\n",
    "            Left_random = random.sample(Left_split,5)\n",
    "            Right_random = random.sample(Right_split,5)\n",
    "            for i in range(5):\n",
    "                f_similar.write(ID_L+\",\"+Left_random[i][0]+\",\"+Left_random[i][1]+\"\\n\")\n",
    "                f_non_similar.write(ID_L+\",\"+Right_random[i][0]+\",\"+Right_random[i][1]+\"\\n\")\n",
    "            \n",
    "f_similar.close()\n",
    "f_non_similar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the statistics of the similar and non similar targets songs for each source song.\n",
    "similars =[]\n",
    "for i in range(len(Target_row_lengths)):\n",
    "    similars.append(Target_row_lengths[i][0])\n",
    "non_similars=[]\n",
    "for i in range(len(Target_row_lengths)):\n",
    "    non_similars.append(Target_row_lengths[i][1])\n",
    "\n",
    "len(Target_row_lengths)\n",
    "import statistics \n",
    "statistics.mean(similars)\n",
    "statistics.stdev(similars)\n",
    "statistics.mean(non_similars)\n",
    "statistics.stdev(non_similars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most similar and non similar target track IDs and their similarity score with source songs.\n",
    "f_similar= open(\"Pairs_labels_similar2.txt\",\"w\")\n",
    "f_non_similar = open(\"Pairs_labels_non_similar2.txt\",\"w\")\n",
    "\n",
    "for row in rows:\n",
    "    ID_L = row[0]\n",
    "    if ID_L not in lyrics_unavailable:\n",
    "        splitted_row = row[1].split(\",\")\n",
    "        k = 0\n",
    "        ID_R_similar = 1\n",
    "        ID_R_non_similar = 1\n",
    "        for i in range(len(splitted_row)):\n",
    "            \n",
    "            if ID_R_similar == 1 and float(splitted_row[k+1])>0.5 and splitted_row[k] not in lyrics_unavailable:\n",
    "                ID_R_similar = splitted_row[k]\n",
    "                score_similar = splitted_row[k+1]\n",
    "            \n",
    "            if ID_R_non_similar == 1 and float(splitted_row[len(splitted_row)-k-1])<=0.5 and splitted_row[len(splitted_row)-k-2] not in lyrics_unavailable:\n",
    "                ID_R_non_similar = splitted_row[len(splitted_row)-k-2]\n",
    "                score_non_similar = splitted_row[len(splitted_row)-k-1]\n",
    "            \n",
    "            if ID_R_similar!=1 and ID_R_non_similar !=1:\n",
    "                f_similar.write(ID_L+\",\"+ID_R_similar+\",\"+score_similar+\"\\n\")\n",
    "                f_non_similar.write(ID_L+\",\"+ID_R_non_similar+\",\"+score_non_similar+\"\\n\")\n",
    "                break\n",
    "            k = k+2\n",
    "            if k>len(splitted_row)-2:\n",
    "                break\n",
    "f_similar.close()\n",
    "f_non_similar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the Track ID pairs based on similarity score\n",
    "def sorted_file(filename):\n",
    "    if filename == \"Pairs_labels_similar2.txt\":\n",
    "        f= open(filename,\"r\").readlines()\n",
    "        output = open(\"sorted_similar_pairs.txt\",\"w\")\n",
    "        for line in sorted(f, key=lambda line: line.split(\",\")[2],reverse = True)[0:35000]:\n",
    "            line = ','.join(line.split(\",\")[0:2])+\",1\\n\"\n",
    "            output.write(line)\n",
    "        output.close()\n",
    "    elif filename ==\"Pairs_labels_non_similar2.txt\":\n",
    "        f= open(filename,\"r\").readlines()\n",
    "        output = open(\"sorted_non_similar_pairs.txt\",\"w\")\n",
    "        for line in sorted(f, key=lambda line: line.split(\",\")[2])[0:35000]:\n",
    "            line = ','.join(line.split(\",\")[0:2])+\",0\\n\"\n",
    "            output.write(line)\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_file(\"Pairs_labels_similar2.txt\")\n",
    "sorted_file(\"Pairs_labels_non_similar2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the files\n",
    "import random\n",
    "seed = 100\n",
    "\n",
    "def shuffle_file(filename):\n",
    "    fid = open(filename, \"r\")\n",
    "    li = fid.readlines()\n",
    "    fid.close()\n",
    "    random.Random(seed).shuffle(li)\n",
    "    fid = open(\"Shuffled_\"+filename, \"w\")\n",
    "    fid.writelines(li)\n",
    "    fid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_file(\"sorted_non_similar_pairs.txt\")\n",
    "shuffle_file(\"sorted_similar_pairs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Generating final dataset</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step in the data processor is to extract the lyrics for all the track ids generated in the previous section and write them to a CSV file as pairs along with their respective similarity scores.\n",
    "<br>\n",
    "The CSV file will have 5 columns:\n",
    "<ul>\n",
    "    <li>X_left_trackid: Track id of the source</li>\n",
    "    <li>X_left: Lyric snippet corresponding to the source</li>\n",
    "    <li>X_right_trackid: Track id of the target</li>\n",
    "    <li>X_right: Lyric snippet corresponding to the target</li>\n",
    "    <li>Y: Similarity score for the pair of lyrics</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking first N data from similar and non-similar data\n",
    "dataset_filepath = \"Project_dataset\"\n",
    "similar_file = \"Pairs_labels_similar.txt\"\n",
    "non_similar_file = \"Pairs_labels_non_similar.txt\"\n",
    "\n",
    "lyrics_files_path = \"Lyrics_dump\" + os.path.sep + \"Lyrics\" + os.path.sep\n",
    "final_dataset = \"final_dataset.csv\"\n",
    "final_dataset_shuffled = \"final_dataset_shuffled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_pairs = []\n",
    "f = open(similar_file)\n",
    "for entry in f:\n",
    "    similar_pairs.append(entry.strip())\n",
    "f.close()\n",
    "\n",
    "non_similar_pairs = []\n",
    "f = open(non_similar_file)\n",
    "for entry in f:\n",
    "    non_similar_pairs.append(entry.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.disable()\n",
    "#Separate CSV files for obtained track pairs\n",
    "dataframe_entries = []\n",
    "processed_trackids = []\n",
    "counter = 0\n",
    "for list_entry in [similar_pairs, non_similar_pairs]:\n",
    "    for entry in list_entry:\n",
    "        entries = entry.split(\",\")\n",
    "        lyrics = []\n",
    "        if os.path.exists(lyrics_files_path + entries[0] + \".txt\") and os.path.exists(lyrics_files_path + entries[1] + \".txt\"):\n",
    "            for i in range(2):\n",
    "                trackid = entries[i]\n",
    "                lyrics_file = lyrics_files_path + trackid + \".txt\"\n",
    "                with open(lyrics_file, \"r\", encoding=\"utf8\") as f:\n",
    "                    curLyrics = f.read()\n",
    "                    curLyrics = curLyrics[:-3].replace(\"\\n\", \" \")\n",
    "                    lyrics.append(curLyrics)\n",
    "            label = 0\n",
    "            if float(entries[2]) >= 0.5:\n",
    "                label = 1\n",
    "            if entries[0] not in processed_trackids:\n",
    "                processed_trackids.append(entries[0])\n",
    "            if entries[1] not in processed_trackids:\n",
    "                processed_trackids.append(entries[1])\n",
    "            dataframe_entries.append([entries[0], lyrics[0], entries[1], lyrics[1], label])\n",
    "        else:\n",
    "            print(\"Unexpected error occurred!\") #All entries in dataset MUST have been verified against availability\n",
    "            print(\"The following trackid pair is unavailable:\", entries)\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print(counter)\n",
    "        \n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_entries, columns = [\"X_left_trackid\", \"X_left\", \"X_right_trackid\", \"X_right\", \"Y\"])\n",
    "display(df.shape)\n",
    "display(df.head())\n",
    "df.to_csv(dataset_filepath + os.path.sep + final_dataset, sep = \"\\t\", encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffle = df.reindex(np.random.permutation(df.index))\n",
    "display(df_shuffle.shape)\n",
    "display(df_shuffle.head())\n",
    "#Write shuffled dataframe to file\n",
    "df_shuffle.to_csv(dataset_filepath + os.path.sep + final_dataset_shuffled, sep = \"\\t\", encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Custom word embeddings</b></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Word2Vec embedding model is generated by training on the lyrics corpus to be used for training the Neural Network.\n",
    "<br><br>\n",
    "This results in a file with all the lyrics named 'fulldump.txt' which is used during the training as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_files_path = \"Lyrics_dump\" + os.path.sep + \"Lyrics\" + os.path.sep\n",
    "\n",
    "#Get the list of all files with lyrics\n",
    "filelist = [f for f in glob.glob(lyrics_files_path + \"*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "otherLangFile = \"lyrics_otherLang.txt\"\n",
    "otherLangFiles = []\n",
    "\n",
    "f = open(otherLangFile, 'r', encoding='utf8')\n",
    "for entry in f:\n",
    "    otherLangFiles.append(entry.split(',')[0])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train_corpus = \"Lyrics_dump\" + os.path.sep + \"fullDump.txt\"\n",
    "\n",
    "dumpFile = open(embedding_train_corpus, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "for entry in filelist:\n",
    "    if entry not in otherLangFiles:\n",
    "        with open(entry, \"r\", encoding=\"utf8\") as f:\n",
    "            curLyrics = f.read()\n",
    "            curLyrics = curLyrics[:-3]\n",
    "            dumpFile.write(curLyrics)\n",
    "\n",
    "dumpFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "embedding_train_corpus = \"Lyrics_dump\" + os.path.sep + \"fullDump.txt\"\n",
    "\n",
    "trainFileObj = open(embedding_train_corpus, \"r\", encoding='utf8')\n",
    "\n",
    "lines = []\n",
    "for entry in trainFileObj:\n",
    "    lineTokens = re.findall(r\"[\\w']+\", entry) #tokenizing the words from the corpus\n",
    "    entry = [''.join(e.lower() for e in x if e.isalpha()) for x in lineTokens]\n",
    "    if len(entry) != 0:\n",
    "        lines.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_model = Word2Vec(size=100, min_count=100, workers=3, window =3, sg = 1)\n",
    "lyrics_model.build_vocab(lines)\n",
    "lyrics_model.train(lines, total_examples=lyrics_model.corpus_count, epochs=200)\n",
    "lyrics_model.wv.save_word2vec_format('lyrics_model_100dim.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_model_50 = Word2Vec(size=50, min_count=100, workers=3, window =3, sg = 1)\n",
    "lyrics_model_50.build_vocab(lines)\n",
    "lyrics_model_50.train(lines, total_examples=lyrics_model_50.corpus_count, epochs=200)\n",
    "lyrics_model_50.wv.save_word2vec_format('lyrics_model_50dim.txt', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_model_10 = Word2Vec(size=10, min_count=100, workers=3, window =3, sg = 1)\n",
    "lyrics_model_10.build_vocab(lines)\n",
    "lyrics_model_10.train(lines, total_examples=lyrics_model_10.corpus_count, epochs=200)\n",
    "lyrics_model.wv.save_word2vec_format('lyrics_model_10dim.txt', binary = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
